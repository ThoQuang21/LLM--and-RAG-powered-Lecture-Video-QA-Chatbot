# =======================================================
# PH·∫¶N 0: IMPORTS
# =======================================================
import os
import chromadb
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, CrossEncoder
from pyvi.ViTokenizer import tokenize
from typing import List, Dict, Any, Optional

# Ki·ªÉm tra xem c√≥ ƒëang ch·∫°y tr√™n Colab kh√¥ng
try:
    from google.colab import drive, userdata
    ON_COLAB = True
except ImportError:
    ON_COLAB = False

# =======================================================
# PH·∫¶N 1: C√ÅC H√ÄM TI·ªÜN √çCH (HELPER FUNCTIONS)
# (Gi·ªØ nguy√™n, kh√¥ng thay ƒë·ªïi)
# =======================================================

def parse_timestamp_to_seconds(ts_str: str) -> int:
    """Chuy·ªÉn ƒë·ªïi timestamp string sang T·ªîNG S·ªê GI√ÇY (int)."""
    if not ts_str or not ts_str.startswith('[') or not ts_str.endswith(']'):
        return 0
    try:
        time_part = ts_str.strip('[]').split(',')[0]
        parts = time_part.split(':')
        total_seconds = 0
        if len(parts) == 3: # HH:MM:SS
            total_seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
        elif len(parts) == 2: # MM:SS
            total_seconds = int(parts[0]) * 60 + int(parts[1])
        elif len(parts) == 1: # SS
            total_seconds = int(parts[0])
        return total_seconds
    except Exception as e:
        print(f"L·ªói khi parse timestamp '{ts_str}': {e}")
        return 0

def clean_youtube_url(url: str) -> str:
    """X√≥a tham s·ªë timestamp (&t=...) ƒë√£ c√≥ kh·ªèi URL."""
    if not url:
        return ""
    return url.split('&t=')[0]

# =======================================================
# PH·∫¶N 2: C√ÅC H√ÄM KH·ªûI T·∫†O (SETUP FUNCTIONS)
# (Gi·ªØ nguy√™n, kh√¥ng thay ƒë·ªïi)
# =======================================================

def ket_noi_google_drive(mount_path: str = '/content/drive') -> bool:
    """K·∫øt n·ªëi v√† mount Google Drive (ch·ªâ d√πng cho Google Colab)."""
    if ON_COLAB:
        print(f"ƒêang k·∫øt n·ªëi v·ªõi Google Drive t·∫°i {mount_path}...")
        try:
            drive.mount(mount_path)
            print("K·∫øt n·ªëi Google Drive th√†nh c√¥ng.")
            return True
        except Exception as e:
            print(f"L·ªói khi k·∫øt n·ªëi Google Drive: {e}")
            return False
    else:
        print("Kh√¥ng ch·∫°y tr√™n Colab, b·ªè qua vi·ªác mount Drive.")
        return True

def tai_model_embedding(model_name: str) -> Optional[SentenceTransformer]:
    """T·∫£i v√† tr·∫£ v·ªÅ model SentenceTransformer."""
    print(f"ƒêang t·∫£i model embedding: {model_name}...")
    try:
        model = SentenceTransformer(model_name)
        print("T·∫£i model embedding th√†nh c√¥ng.")
        return model
    except Exception as e:
        print(f"L·ªói khi t·∫£i model embedding: {e}")
        return None

def tai_model_reranker(model_name: str) -> Optional[CrossEncoder]:
    """T·∫£i v√† tr·∫£ v·ªÅ model CrossEncoder."""
    print(f"ƒêang t·∫£i m√¥ h√¨nh Re-ranker: {model_name}...")
    try:
        reranker = CrossEncoder(model_name)
        print("T·∫£i m√¥ h√¨nh Re-ranker th√†nh c√¥ng.")
        return reranker
    except Exception as e:
        print(f"L·ªói khi t·∫£i Re-ranker: {e}")
        return None

def ket_noi_chromadb(db_path: str, collection_name: str) -> Optional[chromadb.Collection]:
    """K·∫øt n·ªëi ƒë·∫øn Persistent ChromaDB v√† l·∫•y collection."""
    print(f"ƒêang k·∫øt n·ªëi t·ªõi ChromaDB t·∫°i: {db_path}")
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(name=collection_name)
        print(f"T·∫£i th√†nh c√¥ng collection '{collection_name}'. T·ªïng s·ªë m·ª•c: {collection.count()}")
        return collection
    except Exception as e:
        print(f"L·ªói: Kh√¥ng th·ªÉ k·∫øt n·ªëi ho·∫∑c t√¨m th·∫•y collection '{collection_name}'.")
        print(f"Chi ti·∫øt l·ªói: {e}")
        return None

def thiet_lap_gemini(api_key: str) -> Optional[genai.GenerativeModel]:
    """C·∫•u h√¨nh API key cho Gemini v√† tr·∫£ v·ªÅ m·ªôt model."""
    print("ƒêang thi·∫øt l·∫≠p Gemini...")
    if not api_key:
        print("L·ªói: Kh√¥ng t√¨m th·∫•y API Key c·ªßa Gemini.")
        return None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash') 
        print("Thi·∫øt l·∫≠p Gemini th√†nh c√¥ng.")
        return model
    except Exception as e:
        print(f"L·ªói khi c·∫•u h√¨nh Gemini: {e}")
        return None

# =======================================================
# PH·∫¶N 3: H√ÄM PIPELINE RAG CH√çNH (ƒê√É T·ªêI ∆ØU H√ìA)
# =======================================================

def get_rag_answer_pipeline(
    original_query_text: str,
    llm_model: genai.GenerativeModel,
    embedding_model: SentenceTransformer,
    collection: chromadb.Collection,
    reranker: CrossEncoder,
    # C·∫•u h√¨nh hi·ªáu su·∫•t (Performance Tuning)
    per_query_k: int = 7,
    max_docs_to_rerank: int = 25, # <-- [THAY ƒê·ªîI] Gi·ªõi h·∫°n s·ªë docs re-rank
    rerank_batch_size: int = 32,  # <-- [THAY ƒê·ªîI] Batch size cho re-ranker
    score_threshold: float = 0.0,
    min_docs_needed: int = 2,
    fallback_k: int = 3
) -> Dict[str, Optional[str]]:
    """
    Th·ª±c thi to√†n b·ªô pipeline RAG ph·ª©c t·∫°p (Giai ƒëo·∫°n 1-9),
    ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho Batch Retrieval v√† Reranking.
    """
    
    print(f"\n==============================================")
    print(f"üöÄ B·∫ÆT ƒê·∫¶U PIPELINE RAG CHO TRUY V·∫§N: '{original_query_text}'")
    print(f"==============================================")

    # --- GIAI ƒêO·∫†N 2: BI·∫æN ƒê·ªîI TRUY V·∫§N (HyDE + Multi-Query) ---
    # (Gi·ªØ nguy√™n)
    print("\n--- Giai ƒëo·∫°n 2: ƒêang bi·∫øn ƒë·ªïi truy v·∫•n... ---")
    
    # 2A. MULTI-QUERY
    transform_prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch truy v·∫•n.
H√£y ƒë·ªçc c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√† ph√¢n r√£ n√≥ th√†nh 3 c√¢u h·ªèi con, m·ªói c√¢u h·ªèi khai th√°c m·ªôt kh√≠a c·∫°nh kh√°c nhau (ƒë·ªãnh nghƒ©a, b·∫£n ch·∫•t, m·ª•c ƒë√≠ch).
**Y√™u c·∫ßu:** Ch·ªâ tr·∫£ l·ªùi b·∫±ng c√°c c√¢u h·ªèi con, m·ªói c√¢u h·ªèi tr√™n m·ªôt d√≤ng.
**C√¢u h·ªèi g·ªëc:** "{original_query_text}"
**C√°c c√¢u h·ªèi con (ph√¢n r√£):**
"""
    transform_config = genai.types.GenerationConfig(temperature=0.0)
    generated_queries = []
    try:
        transform_response = llm_model.generate_content(transform_prompt, generation_config=transform_config)
        sub_queries_text = transform_response.text
        generated_queries = [q.strip() for q in sub_queries_text.split('\n') if q.strip()]
    except Exception as e:
        print(f"L·ªói khi t·∫°o sub-query: {e}")

    # 2B. HYDE
    print("ƒêang t·∫°o t√†i li·ªáu gi·∫£ l·∫≠p (HyDE)...")
    hyde_prompt = f"""
H√£y vi·∫øt m·ªôt ƒëo·∫°n vƒÉn ng·∫Øn (kho·∫£ng 2-3 c√¢u) tr·∫£ l·ªùi tr·ª±c ti·∫øp cho c√¢u h·ªèi sau.
ƒêo·∫°n vƒÉn n√†y s·∫Ω ƒë∆∞·ª£c d√πng ƒë·ªÉ t√¨m ki·∫øm c√°c t√†i li·ªáu t∆∞∆°ng t·ª±.
H√£y t·∫≠p trung v√†o c√°c t·ª´ kh√≥a v√† kh√°i ni·ªám c·ªët l√µi.
**C√¢u h·ªèi:** "{original_query_text}"
**C√¢u tr·∫£ l·ªùi gi·∫£ l·∫≠p:**
"""
    hyde_config = genai.types.GenerationConfig(temperature=0.3)
    hyde_document_text = ""
    try:
        hyde_response = llm_model.generate_content(hyde_prompt, generation_config=hyde_config)
        hyde_document_text = hyde_response.text.strip().replace("\n", " ")
    except Exception as e:
        print(f"L·ªói khi t·∫°o HyDE doc: {e}")

    # 2C. T·ªîNG H·ª¢P
    all_search_texts = [original_query_text] + generated_queries
    if hyde_document_text:
        all_search_texts.append(hyde_document_text)
    print(f"ƒê√£ t·∫°o {len(all_search_texts)} vƒÉn b·∫£n ƒë·ªÉ t√¨m ki·∫øm (g·ªëc + con + HyDE).")


    # --- GIAI ƒêO·∫†N 3: RETRIEVAL (T·ªêI ∆ØU H√ìA V·ªöI BATCH) ---
    print(f"\n--- Giai ƒëo·∫°n 3: ƒêang truy xu·∫•t (Batch) cho {len(all_search_texts)} vƒÉn b·∫£n... ---")
    all_retrieved_docs_map = {}
    
    try:
        # 1. Tokenize t·∫•t c·∫£ vƒÉn b·∫£n
        tokenized_queries = [tokenize(text) for text in all_search_texts]
        
        # 2. Encode t·∫•t c·∫£ trong m·ªôt batch
        # (SentenceTransformer t·ª± ƒë·ªông x·ª≠ l√Ω batching b√™n trong)
        print(f"ƒêang batch-encode {len(tokenized_queries)} vƒÉn b·∫£n...")
        search_embeddings = embedding_model.encode(tokenized_queries).tolist()
        
        # 3. Truy v·∫•n ChromaDB m·ªôt l·∫ßn duy nh·∫•t (batch query)
        print(f"ƒêang batch-query ChromaDB (k={per_query_k} m·ªói vƒÉn b·∫£n)...")
        all_results = collection.query(
            query_embeddings=search_embeddings,
            n_results=per_query_k,
            include=["metadatas", "documents"]
        )

        # 4. G·ªôp (Flatten) v√† lo·∫°i b·ªè tr√πng l·∫∑p
        list_of_doc_lists = all_results.get('documents', [])
        list_of_meta_lists = all_results.get('metadatas', [])

        for doc_list, meta_list in zip(list_of_doc_lists, list_of_meta_lists):
            for doc_text, meta in zip(doc_list, meta_list):
                if doc_text not in all_retrieved_docs_map:
                    all_retrieved_docs_map[doc_text] = meta
                    
    except Exception as e:
        print(f"L·ªói nghi√™m tr·ªçng trong Giai ƒëo·∫°n 3 (Batch Retrieval): {e}")

    retrieved_doc_texts = list(all_retrieved_docs_map.keys())
    
    if not retrieved_doc_texts:
        print("L·ªói: Kh√¥ng truy xu·∫•t ƒë∆∞·ª£c b·∫•t k·ª≥ t√†i li·ªáu n√†o. D·ª´ng pipeline.")
        return {"answer": "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o li√™n quan ƒë·∫øn c√¢u h·ªèi n√†y.", "source_url": None}
        
    print(f"ƒê√£ truy xu·∫•t ƒë∆∞·ª£c t·ªïng c·ªông {len(retrieved_doc_texts)} chunks (duy nh·∫•t).")


    # --- GIAI ƒêO·∫†N 4: RE-RANKING (T·ªêI ∆ØU H√ìA V·ªöI GI·ªöI H·∫†N & BATCH) ---
    
    # 4a. √Åp d·ª•ng trade-off: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng docs
    if len(retrieved_doc_texts) > max_docs_to_rerank:
        print(f"\n--- Giai ƒëo·∫°n 4: ƒêang Re-ranking (GI·ªöI H·∫†N c√≤n {max_docs_to_rerank} / {len(retrieved_doc_texts)} chunks)... ---")
        docs_to_rerank = retrieved_doc_texts[:max_docs_to_rerank]
    else:
        print(f"\n--- Giai ƒëo·∫°n 4: ƒêang Re-ranking {len(retrieved_doc_texts)} chunks... ---")
        docs_to_rerank = retrieved_doc_texts

    # 4b. Chu·∫©n b·ªã c·∫∑p [query, doc]
    query_chunk_pairs = [[original_query_text, doc_text] for doc_text in docs_to_rerank]
    
    try:
        # 4c. D·ª± ƒëo√°n theo batch (explicitly)
        print(f"ƒêang d·ª± ƒëo√°n ƒëi·ªÉm re-rank (batch_size={rerank_batch_size})...")
        scores = reranker.predict(
            query_chunk_pairs, 
            batch_size=rerank_batch_size
        )
        
        # 4d. S·∫Øp x·∫øp
        metadatas_to_rerank = [all_retrieved_docs_map[text] for text in docs_to_rerank]
        scored_chunks_with_meta = sorted(
            list(zip(scores, docs_to_rerank, metadatas_to_rerank)),
            key=lambda x: x[0],
            reverse=True
        )
        print("ƒê√£ ch·∫•m ƒëi·ªÉm v√† s·∫Øp x·∫øp xong.")
        
    except Exception as e:
        print(f"L·ªói khi re-ranking: {e}")
        # Fallback: D√πng t·∫°m c√°c chunks ch∆∞a re-rank n·∫øu reranker l·ªói
        scored_chunks_with_meta = [(0.0, text, all_retrieved_docs_map[text]) for text in docs_to_rerank]


    # --- GIAI ƒêO·∫†N 4.5: TR√çCH XU·∫§T LINK T·ªêT NH·∫§T ---
    # (Gi·ªØ nguy√™n)
    best_source_url = None
    if scored_chunks_with_meta:
        top_score, top_text, top_meta = scored_chunks_with_meta[0]
        if top_meta and 'source_url' in top_meta and 'start_time' in top_meta:
            original_url = top_meta['source_url']
            start_time_str = top_meta['start_time']
            base_url = clean_youtube_url(original_url)
            total_seconds = parse_timestamp_to_seconds(start_time_str)
            if total_seconds > 0:
                best_source_url = f"{base_url}&t={total_seconds}s"
            else:
                best_source_url = base_url
            print(f"ƒê√£ tr√≠ch xu·∫•t ngu·ªìn t·ªët nh·∫•t: {best_source_url}")
        elif top_meta and 'source_url' in top_meta:
            best_source_url = top_meta['source_url']
            print(f"ƒê√£ tr√≠ch xu·∫•t ngu·ªìn (kh√¥ng c√≥ start_time): {best_source_url}")
        else:
            print("Kh√¥ng t√¨m th·∫•y 'source_url' trong metadata c·ªßa H·∫°ng 1.")
    else:
        print("Kh√¥ng c√≥ chunk n√†o ƒë·ªÉ tr√≠ch xu·∫•t link.")


    # --- GIAI ƒêO·∫†N 4d: L·ªåC ---
    # (Gi·ªØ nguy√™n)
    threshold_chunks = [item for item in scored_chunks_with_meta if item[0] >= score_threshold]
    if len(threshold_chunks) < min_docs_needed:
        print(f"Ch·ªâ t√¨m th·∫•y {len(threshold_chunks)} chunk v∆∞·ª£t ng∆∞·ª°ng. Fallback v·ªÅ Top-{fallback_k}.")
        final_best_chunks = scored_chunks_with_meta[:fallback_k]
    else:
        print(f"ƒê√£ t√¨m th·∫•y {len(threshold_chunks)} chunk v∆∞·ª£t ng∆∞·ª°ng (ƒë·ªß y√™u c·∫ßu).")
        final_best_chunks = threshold_chunks
    print(f"T·ªïng c·ªông s·∫Ω d√πng {len(final_best_chunks)} chunk (ƒë√£ l·ªçc) ƒë·ªÉ l·∫•y context.")


    # --- GIAI ƒêO·∫†N 5: X√¢y d·ª±ng Context ---
    # (Gi·ªØ nguy√™n)
    final_context_windows_texts = []
    for score, text, meta in final_best_chunks:
        if meta and 'context_window' in meta:
            final_context_windows_texts.append(meta['context_window'])
        else:
            final_context_windows_texts.append(text)
    unique_contexts = list(dict.fromkeys(final_context_windows_texts))
    context_string = "\n\n".join(unique_contexts)


    # --- GIAI ƒêO·∫†N 6: PROMPT ---
    # (Gi·ªØ nguy√™n)
    strict_prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI, nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi CH·ªà D·ª∞A TR√äN b·ªëi c·∫£nh (context) ƒë∆∞·ª£c cung c·∫•p.
H√£y ƒë·ªçc k·ªπ ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y:
--- (B·∫ÆT ƒê·∫¶U VƒÇN B·∫¢N) ---
{context_string}
--- (K·∫æT TH√öC VƒÇN B·∫¢N) ---
D·ª±a tr√™n vƒÉn b·∫£n tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau:
C√¢u h·ªèi: {original_query_text}
**Y√™u c·∫ßu nghi√™m ng·∫∑t:**
1. ƒê·ªçc k·ªπ b·ªëi c·∫£nh ƒë√£ cho.
2. C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n PH·∫¢I ƒë∆∞·ª£c r√∫t ra tr·ª±c ti·∫øp t·ª´ th√¥ng tin c√≥ trong b·ªëi c·∫£nh.
3. **QUAN TR·ªåNG:** N·∫øu th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi kh√¥ng c√≥ trong b·ªëi c·∫£nh, h√£y tr·∫£ l·ªùi ch√≠nh x√°c m·ªôt c√¢u: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong b·ªëi c·∫£nh ƒë∆∞·ª£c cung c·∫•p."
4. Kh√¥ng ƒë∆∞·ª£c suy di·ªÖn, kh√¥ng ƒë∆∞·ª£c th√™m ki·∫øn th·ª©c b√™n ngo√†i, kh√¥ng ƒë∆∞·ª£c "ch√©m gi√≥".
**C√¢u tr·∫£ l·ªùi (d·ª±a tr√™n b·ªëi c·∫£nh):**
"""

    # --- GIAI ƒêO·∫†N 7: C·∫•u h√¨nh Generation ---
    # (Gi·ªØ nguy√™n)
    config = genai.types.GenerationConfig(temperature=0.1)
    safety_settings = [
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]

    # --- GIAI ƒêO·∫†N 8 & 9: GENERATE v√† Tr·∫£ v·ªÅ k·∫øt qu·∫£ ---
    # (Gi·ªØ nguy√™n)
    print("\n--- Giai ƒëo·∫°n 8: ƒêang g·ª≠i prompt nghi√™m ng·∫∑t ƒë·∫øn Gemini... ---")
    final_answer = ""
    try:
        response = llm_model.generate_content(
            strict_prompt,
            generation_config=config,
            safety_settings=safety_settings
        )
        final_answer = response.text.strip()
        print("ƒê√£ nh·∫≠n ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi.")
    except Exception as e:
        print(f"X·∫£y ra l·ªói khi g·ªçi Gemini: {e}")
        final_answer = f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói trong qu√° tr√¨nh t·∫°o c√¢u tr·∫£ l·ªùi: {e}"

    print("üèÅ PIPELINE RAG HO√ÄN T·∫§T.")
    
    return {
        "answer": final_answer,
        "source_url": best_source_url
    }

# =======================================================
# PH·∫¶N 4: H√ÄM MAIN TH·ª∞C THI (ƒê√É C·∫¨P NH·∫¨T)
# =======================================================

def main():
    """
    H√†m th·ª±c thi ch√≠nh:
    1. Thi·∫øt l·∫≠p h·∫±ng s·ªë (ƒë∆∞·ªùng d·∫´n, t√™n model).
    2. L·∫•y API Key.
    3. G·ªçi c√°c h√†m trong PH·∫¶N 2 ƒë·ªÉ kh·ªüi t·∫°o t·∫•t c·∫£ th√†nh ph·∫ßn.
    4. Ki·ªÉm tra l·ªói.
    5. ƒê·∫∑t c√¢u h·ªèi v√† g·ªçi h√†m pipeline (PH·∫¶N 3) v·ªõi c√°c tham s·ªë t·ªëi ∆∞u h√≥a.
    6. In k·∫øt qu·∫£ cu·ªëi c√πng.
    """
    
    # --- 1. Thi·∫øt l·∫≠p h·∫±ng s·ªë ---
    print("--- ‚öôÔ∏è B·∫ÆT ƒê·∫¶U KH·ªûI CH·∫†Y RAG PIPELINE (T·ªêI ∆ØU H√ìA) ‚öôÔ∏è ---")
    DB_PATH = "my_rag_db_2"
    COLLECTION_NAME = "bai_giang_videos"
    EMBEDDING_MODEL_NAME = 'VoVanPhuc/sup-SimCSE-VietNamese-phobert-base'
    RERANKER_MODEL_NAME = 'cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
    DRIVE_MOUNT_PATH = '/content/drive'

    # --- 2. L·∫•y API Key (An to√†n) ---
    GEMINI_API_KEY = 'AIzaSyAqJn039l1ThNaNATJ_4wTIgHv0hrxKRWE'


    if not GEMINI_API_KEY:
        print("\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y 'GOOGLE_API_KEY'.")
        print("Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn n√†y trong Colab Secrets ho·∫∑c m√¥i tr∆∞·ªùng c·ªßa b·∫°n.")
        return

    # --- 3. Kh·ªüi t·∫°o t·∫•t c·∫£ th√†nh ph·∫ßn ---
    if ON_COLAB:
        ket_noi_google_drive(DRIVE_MOUNT_PATH)
    
    model_embed = tai_model_embedding(EMBEDDING_MODEL_NAME)
    model_rerank = tai_model_reranker(RERANKER_MODEL_NAME)
    rag_collection = ket_noi_chromadb(DB_PATH, COLLECTION_NAME)
    llm_model = thiet_lap_gemini(GEMINI_API_KEY)

    # --- 4. Ki·ªÉm tra ---
    all_components = {
        "Embedding Model": model_embed,
        "Reranker Model": model_rerank,
        "ChromaDB Collection": rag_collection,
        "LLM Model": llm_model
    }
    
    if not all(all_components.values()):
        print("\n‚ùå L·ªñI: M·ªôt ho·∫∑c nhi·ªÅu th√†nh ph·∫ßn kh√¥ng th·ªÉ kh·ªüi t·∫°o.")
        for name, component in all_components.items():
            if not component:
                print(f"    - {name}: KH·ªûI T·∫†O TH·∫§T B·∫†I")
        print("Vui l√≤ng ki·ªÉm tra l·∫°i l·ªói b√™n tr√™n v√† ƒë∆∞·ªùng d·∫´n. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return
        
    print("\n===================================")
    print("‚úÖ T·∫§T C·∫¢ MODEL V√Ä DB ƒê√É S·∫¥N S√ÄNG!")
    print("===================================")

    # --- 5. ƒê·∫∑t c√¢u h·ªèi v√† ch·∫°y pipeline (V·ªöI THAM S·ªê T·ªêI ∆ØU H√ìA) ---
    
    query = "Attention kh√°c Self-Attention ·ªü ƒëi·ªÉm n√†o"
    
    result = get_rag_answer_pipeline(
        original_query_text=query,
        llm_model=llm_model,
        embedding_model=model_embed,
        collection=rag_collection,
        reranker=model_rerank,
        # ‚ñº‚ñº‚ñº Tham s·ªë t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô ‚ñº‚ñº‚ñº
        per_query_k=5,              # Gi·∫£m K m·ªôt ch√∫t (v√≠ d·ª•: 5 thay v√¨ 7)
        max_docs_to_rerank=20,      # Ch·ªâ re-rank 20 docs h√†ng ƒë·∫ßu
        rerank_batch_size=32        # Ch·∫°y re-ranker v·ªõi batch-size 32
    )

    # --- 6. In k·∫øt qu·∫£ cu·ªëi c√πng ---
    print("\n\n================ K·∫æT QU·∫¢ CU·ªêI C√ôNG ================")
    print(f"‚ùì H·ªéI: {query}\n")
    print(f"ü§ñ TR·∫¢ L·ªúI:\n{result.get('answer', 'Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi')}\n")
    print(f"üîó NGU·ªíN THAM KH·∫¢O:\n{result.get('source_url', 'Kh√¥ng c√≥ ngu·ªìn')}")
    print("==================================================")

# =======================================================
# PH·∫¶N 5: ƒêI·ªÇM B·∫ÆT ƒê·∫¶U CH·∫†Y CODE
# =======================================================
if __name__ == "__main__":
    main()