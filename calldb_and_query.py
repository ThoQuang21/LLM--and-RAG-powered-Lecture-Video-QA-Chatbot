# =======================================================
# PHáº¦N 0: IMPORTS
# =======================================================
import os
import chromadb
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, CrossEncoder
from pyvi.ViTokenizer import tokenize
from typing import List, Dict, Any, Optional

# Kiá»ƒm tra xem cÃ³ Ä‘ang cháº¡y trÃªn Colab khÃ´ng
try:
    from google.colab import drive, userdata
    ON_COLAB = True
except ImportError:
    ON_COLAB = False

# =======================================================
# PHáº¦N 1: CÃC HÃ€M TIá»†N ÃCH (HELPER FUNCTIONS)
# =======================================================

def parse_timestamp_to_seconds(ts_str: str) -> int:
    """
    Chuyá»ƒn Ä‘á»•i timestamp string [HH:MM:SS,ms] hoáº·c [MM:SS,ms]
    sang Tá»”NG Sá» GIÃ‚Y (int).
    """
    if not ts_str or not ts_str.startswith('[') or not ts_str.endswith(']'):
        return 0
    try:
        time_part = ts_str.strip('[]').split(',')[0]
        parts = time_part.split(':')
        total_seconds = 0
        if len(parts) == 3: # HH:MM:SS
            total_seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
        elif len(parts) == 2: # MM:SS
            total_seconds = int(parts[0]) * 60 + int(parts[1])
        elif len(parts) == 1: # SS
            total_seconds = int(parts[0])
        return total_seconds
    except Exception as e:
        print(f"Lá»—i khi parse timestamp '{ts_str}': {e}")
        return 0

def clean_youtube_url(url: str) -> str:
    """
    XÃ³a tham sá»‘ timestamp (&t=...) Ä‘Ã£ cÃ³ khá»i URL YouTube.
    """
    if not url:
        return ""
    return url.split('&t=')[0]

# =======================================================
# PHáº¦N 2: CÃC HÃ€M KHá»žI Táº O (SETUP FUNCTIONS)
# =======================================================

def ket_noi_google_drive(mount_path: str = '/content/drive') -> bool:
    """
    Káº¿t ná»‘i vÃ  mount Google Drive (chá»‰ dÃ¹ng cho Google Colab).
    """
    if ON_COLAB:
        print(f"Äang káº¿t ná»‘i vá»›i Google Drive táº¡i {mount_path}...")
        try:
            drive.mount(mount_path)
            print("Káº¿t ná»‘i Google Drive thÃ nh cÃ´ng.")
            return True
        except Exception as e:
            print(f"Lá»—i khi káº¿t ná»‘i Google Drive: {e}")
            return False
    else:
        print("KhÃ´ng cháº¡y trÃªn Colab, bá» qua viá»‡c mount Drive.")
        return True

def tai_model_embedding(model_name: str) -> Optional[SentenceTransformer]:
    """
    Táº£i vÃ  tráº£ vá» model SentenceTransformer Ä‘á»ƒ táº¡o embedding.
    """
    print(f"Äang táº£i model embedding: {model_name}...")
    try:
        model = SentenceTransformer(model_name)
        print("Táº£i model embedding thÃ nh cÃ´ng.")
        return model
    except Exception as e:
        print(f"Lá»—i khi táº£i model embedding: {e}")
        return None

def tai_model_reranker(model_name: str) -> Optional[CrossEncoder]:
    """
    Táº£i vÃ  tráº£ vá» model CrossEncoder Ä‘á»ƒ re-ranking.
    (Dá»±a trÃªn code má»›i nháº¥t báº¡n cung cáº¥p)
    """
    print(f"Äang táº£i mÃ´ hÃ¬nh Re-ranker: {model_name}...")
    try:
        reranker = CrossEncoder(model_name)
        print("Táº£i mÃ´ hÃ¬nh Re-ranker thÃ nh cÃ´ng.")
        return reranker
    except Exception as e:
        print(f"Lá»—i khi táº£i Re-ranker: {e}")
        return None

def ket_noi_chromadb(db_path: str, collection_name: str) -> Optional[chromadb.Collection]:
    """
    Káº¿t ná»‘i Ä‘áº¿n Persistent ChromaDB vÃ  láº¥y collection Ä‘Ã£ tá»“n táº¡i.
    """
    print(f"Äang káº¿t ná»‘i tá»›i ChromaDB táº¡i: {db_path}")
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(name=collection_name)
        print(f"Táº£i thÃ nh cÃ´ng collection '{collection_name}'. Tá»•ng sá»‘ má»¥c: {collection.count()}")
        return collection
    except Exception as e:
        print(f"Lá»—i: KhÃ´ng thá»ƒ káº¿t ná»‘i hoáº·c tÃ¬m tháº¥y collection '{collection_name}'.")
        print(f"Chi tiáº¿t lá»—i: {e}")
        return None

def thiet_lap_gemini(api_key: str) -> Optional[genai.GenerativeModel]:
    """
    Cáº¥u hÃ¬nh API key cho Gemini vÃ  tráº£ vá» má»™t model cÃ³ thá»ƒ sá»­ dá»¥ng.
    """
    print("Äang thiáº¿t láº­p Gemini...")
    if not api_key:
        print("Lá»—i: KhÃ´ng tÃ¬m tháº¥y API Key cá»§a Gemini.")
        return None
    try:
        genai.configure(api_key='AIzaSyBD27hwT7Zu1yACDlbR1sEoVDKww2T2Cuo')
        model = genai.GenerativeModel('gemini-2.5-flash') 
        print("Thiáº¿t láº­p Gemini thÃ nh cÃ´ng.")
        return model
    except Exception as e:
        print(f"Lá»—i khi cáº¥u hÃ¬nh Gemini: {e}")
        return None

# =======================================================
# PHáº¦N 3: HÃ€M PIPELINE RAG CHÃNH (GIAI ÄOáº N 1-9)
# =======================================================

def get_rag_answer_pipeline(
    original_query_text: str,
    llm_model: genai.GenerativeModel,
    embedding_model: SentenceTransformer,
    collection: chromadb.Collection,
    reranker: CrossEncoder,
    # Cáº¥u hÃ¬nh cÃ³ thá»ƒ Ä‘iá»u chá»‰nh
    per_query_k: int = 7,
    score_threshold: float = 0.0,
    min_docs_needed: int = 2,
    fallback_k: int = 3
) -> Dict[str, Optional[str]]:
    """
    Thá»±c thi toÃ n bá»™ pipeline RAG phá»©c táº¡p (Giai Ä‘oáº¡n 1-9).
    """
    
    print(f"\n==============================================")
    print(f"ðŸš€ Báº®T Äáº¦U PIPELINE RAG CHO TRUY Váº¤N: '{original_query_text}'")
    print(f"==============================================")

    # --- GIAI ÄOáº N 2: BIáº¾N Äá»”I TRUY Váº¤N (HyDE + Multi-Query) ---
    print("\n--- Giai Ä‘oáº¡n 2: Äang biáº¿n Ä‘á»•i truy váº¥n... ---")
    
    # 2A. MULTI-QUERY
    transform_prompt = f"""
Báº¡n lÃ  má»™t chuyÃªn gia phÃ¢n tÃ­ch truy váº¥n.
HÃ£y Ä‘á»c cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vÃ  phÃ¢n rÃ£ nÃ³ thÃ nh 3 cÃ¢u há»i con, má»—i cÃ¢u há»i khai thÃ¡c má»™t khÃ­a cáº¡nh khÃ¡c nhau (Ä‘á»‹nh nghÄ©a, báº£n cháº¥t, má»¥c Ä‘Ã­ch).
**YÃªu cáº§u:** Chá»‰ tráº£ lá»i báº±ng cÃ¡c cÃ¢u há»i con, má»—i cÃ¢u há»i trÃªn má»™t dÃ²ng.
**CÃ¢u há»i gá»‘c:** "{original_query_text}"
**CÃ¡c cÃ¢u há»i con (phÃ¢n rÃ£):**
"""
    transform_config = genai.types.GenerationConfig(temperature=0.0)
    generated_queries = []
    try:
        transform_response = llm_model.generate_content(transform_prompt, generation_config=transform_config)
        sub_queries_text = transform_response.text
        generated_queries = [q.strip() for q in sub_queries_text.split('\n') if q.strip()]
    except Exception as e:
        print(f"Lá»—i khi táº¡o sub-query: {e}")

    # 2B. HYDE
    print("Äang táº¡o tÃ i liá»‡u giáº£ láº­p (HyDE)...")
    hyde_prompt = f"""
HÃ£y viáº¿t má»™t Ä‘oáº¡n vÄƒn ngáº¯n (khoáº£ng 2-3 cÃ¢u) tráº£ lá»i trá»±c tiáº¿p cho cÃ¢u há»i sau.
Äoáº¡n vÄƒn nÃ y sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tÃ¬m kiáº¿m cÃ¡c tÃ i liá»‡u tÆ°Æ¡ng tá»±.
HÃ£y táº­p trung vÃ o cÃ¡c tá»« khÃ³a vÃ  khÃ¡i niá»‡m cá»‘t lÃµi.
**CÃ¢u há»i:** "{original_query_text}"
**CÃ¢u tráº£ lá»i giáº£ láº­p:**
"""
    hyde_config = genai.types.GenerationConfig(temperature=0.3)
    hyde_document_text = ""
    try:
        hyde_response = llm_model.generate_content(hyde_prompt, generation_config=hyde_config)
        hyde_document_text = hyde_response.text.strip().replace("\n", " ")
    except Exception as e:
        print(f"Lá»—i khi táº¡o HyDE doc: {e}")

    # 2C. Tá»”NG Há»¢P
    all_search_texts = [original_query_text] + generated_queries
    if hyde_document_text:
        all_search_texts.append(hyde_document_text)
    print(f"ÄÃ£ táº¡o {len(all_search_texts)} vÄƒn báº£n Ä‘á»ƒ tÃ¬m kiáº¿m (gá»‘c + con + HyDE).")


    # --- GIAI ÄOáº N 3: RETRIEVAL ---
    all_retrieved_docs_map = {}
    print(f"\n--- Giai Ä‘oáº¡n 3: Äang truy xuáº¥t cho {len(all_search_texts)} vÄƒn báº£n (k={per_query_k} má»—i vÄƒn báº£n)... ---")

    for search_text in all_search_texts:
        try:
            search_tokenized = tokenize(search_text)
            search_embedding = embedding_model.encode([search_tokenized])
            results = collection.query(
                query_embeddings=search_embedding.tolist(),
                n_results=per_query_k,
                include=["metadatas", "documents"]
            )
            documents = results['documents'][0]
            metadatas = results['metadatas'][0]
            for doc_text, meta in zip(documents, metadatas):
                if doc_text not in all_retrieved_docs_map:
                    all_retrieved_docs_map[doc_text] = meta
        except Exception as e:
            print(f"Lá»—i khi truy xuáº¥t cho vÄƒn báº£n '{search_text[:50]}...': {e}")

    retrieved_doc_texts = list(all_retrieved_docs_map.keys())
    if not retrieved_doc_texts:
        print("Lá»—i: KhÃ´ng truy xuáº¥t Ä‘Æ°á»£c báº¥t ká»³ tÃ i liá»‡u nÃ o. Dá»«ng pipeline.")
        return {"answer": "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u nÃ o liÃªn quan Ä‘áº¿n cÃ¢u há»i nÃ y.", "source_url": None}
    print(f"ÄÃ£ truy xuáº¥t Ä‘Æ°á»£c tá»•ng cá»™ng {len(retrieved_doc_texts)} chunks (duy nháº¥t).")


    # --- GIAI ÄOáº N 4: RE-RANKING ---
    print(f"\n--- Giai Ä‘oáº¡n 4: Äang Re-ranking {len(retrieved_doc_texts)} chunks... ---")
    query_chunk_pairs = [[original_query_text, doc_text] for doc_text in retrieved_doc_texts]
    try:
        scores = reranker.predict(query_chunk_pairs)
        retrieved_metadatas = [all_retrieved_docs_map[text] for text in retrieved_doc_texts]
        scored_chunks_with_meta = sorted(
            list(zip(scores, retrieved_doc_texts, retrieved_metadatas)),
            key=lambda x: x[0],
            reverse=True
        )
        print("ÄÃ£ cháº¥m Ä‘iá»ƒm vÃ  sáº¯p xáº¿p xong.")
    except Exception as e:
        print(f"Lá»—i khi re-ranking: {e}")
        scored_chunks_with_meta = [(0.0, text, all_retrieved_docs_map[text]) for text in retrieved_doc_texts]


    # --- GIAI ÄOáº N 4.5: TRÃCH XUáº¤T LINK Tá»T NHáº¤T ---
    best_source_url = None
    if scored_chunks_with_meta:
        top_score, top_text, top_meta = scored_chunks_with_meta[0]
        if top_meta and 'source_url' in top_meta and 'start_time' in top_meta:
            original_url = top_meta['source_url']
            start_time_str = top_meta['start_time']
            base_url = clean_youtube_url(original_url)
            total_seconds = parse_timestamp_to_seconds(start_time_str)
            if total_seconds > 0:
                best_source_url = f"{base_url}&t={total_seconds}s"
            else:
                best_source_url = base_url
            print(f"ÄÃ£ trÃ­ch xuáº¥t nguá»“n tá»‘t nháº¥t: {best_source_url}")
        elif top_meta and 'source_url' in top_meta:
            best_source_url = top_meta['source_url']
            print(f"ÄÃ£ trÃ­ch xuáº¥t nguá»“n (khÃ´ng cÃ³ start_time): {best_source_url}")
        else:
            print("KhÃ´ng tÃ¬m tháº¥y 'source_url' trong metadata cá»§a Háº¡ng 1.")
    else:
        print("KhÃ´ng cÃ³ chunk nÃ o Ä‘á»ƒ trÃ­ch xuáº¥t link.")


    # --- GIAI ÄOáº N 4d: Lá»ŒC ---
    threshold_chunks = [item for item in scored_chunks_with_meta if item[0] >= score_threshold]
    if len(threshold_chunks) < min_docs_needed:
        print(f"Chá»‰ tÃ¬m tháº¥y {len(threshold_chunks)} chunk vÆ°á»£t ngÆ°á»¡ng. Fallback vá» Top-{fallback_k}.")
        final_best_chunks = scored_chunks_with_meta[:fallback_k]
    else:
        print(f"ÄÃ£ tÃ¬m tháº¥y {len(threshold_chunks)} chunk vÆ°á»£t ngÆ°á»¡ng (Ä‘á»§ yÃªu cáº§u).")
        final_best_chunks = threshold_chunks
    print(f"Tá»•ng cá»™ng sáº½ dÃ¹ng {len(final_best_chunks)} chunk (Ä‘Ã£ lá»c) Ä‘á»ƒ láº¥y context.")


    # --- GIAI ÄOáº N 5: XÃ¢y dá»±ng Context ---
    final_context_windows_texts = []
    for score, text, meta in final_best_chunks:
        if meta and 'context_window' in meta:
            final_context_windows_texts.append(meta['context_window'])
        else:
            final_context_windows_texts.append(text)
    unique_contexts = list(dict.fromkeys(final_context_windows_texts))
    context_string = "\n\n".join(unique_contexts)


    # --- GIAI ÄOáº N 6: PROMPT ---
    strict_prompt = f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI, nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i CHá»ˆ Dá»°A TRÃŠN bá»‘i cáº£nh (context) Ä‘Æ°á»£c cung cáº¥p.
HÃ£y Ä‘á»c ká»¹ Ä‘oáº¡n vÄƒn báº£n dÆ°á»›i Ä‘Ã¢y:
--- (Báº®T Äáº¦U VÄ‚N Báº¢N) ---
{context_string}
--- (Káº¾T THÃšC VÄ‚N Báº¢N) ---
Dá»±a trÃªn vÄƒn báº£n trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i sau:
CÃ¢u há»i: {original_query_text}
**YÃªu cáº§u nghiÃªm ngáº·t:**
1. Äá»c ká»¹ bá»‘i cáº£nh Ä‘Ã£ cho.
2. CÃ¢u tráº£ lá»i cá»§a báº¡n PHáº¢I Ä‘Æ°á»£c rÃºt ra trá»±c tiáº¿p tá»« thÃ´ng tin cÃ³ trong bá»‘i cáº£nh.
3. **QUAN TRá»ŒNG:** Náº¿u thÃ´ng tin Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khÃ´ng cÃ³ trong bá»‘i cáº£nh, hÃ£y tráº£ lá»i chÃ­nh xÃ¡c má»™t cÃ¢u: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong bá»‘i cáº£nh Ä‘Æ°á»£c cung cáº¥p."
4. KhÃ´ng Ä‘Æ°á»£c suy diá»…n, khÃ´ng Ä‘Æ°á»£c thÃªm kiáº¿n thá»©c bÃªn ngoÃ i, khÃ´ng Ä‘Æ°á»£c "chÃ©m giÃ³".
**CÃ¢u tráº£ lá»i (dá»±a trÃªn bá»‘i cáº£nh):**
"""

    # --- GIAI ÄOáº N 7: Cáº¥u hÃ¬nh Generation ---
    config = genai.types.GenerationConfig(temperature=0.1)
    safety_settings = [
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]

    # --- GIAI ÄOáº N 8 & 9: GENERATE vÃ  Tráº£ vá» káº¿t quáº£ ---
    print("\n--- Giai Ä‘oáº¡n 8: Äang gá»­i prompt nghiÃªm ngáº·t Ä‘áº¿n Gemini... ---")
    final_answer = ""
    try:
        response = llm_model.generate_content(
            strict_prompt,
            generation_config=config,
            safety_settings=safety_settings
        )
        final_answer = response.text.strip()
        print("ÄÃ£ nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i.")
    except Exception as e:
        print(f"Xáº£y ra lá»—i khi gá»i Gemini: {e}")
        final_answer = f"Xin lá»—i, Ä‘Ã£ xáº£y ra lá»—i trong quÃ¡ trÃ¬nh táº¡o cÃ¢u tráº£ lá»i: {e}"

    print("ðŸ PIPELINE RAG HOÃ€N Táº¤T.")
    
    return {
        "answer": final_answer,
        "source_url": best_source_url
    }

# =======================================================
# PHáº¦N 4: HÃ€M MAIN THá»°C THI
# =======================================================

def main():
    """
    HÃ m thá»±c thi chÃ­nh:
    1. Thiáº¿t láº­p háº±ng sá»‘ (Ä‘Æ°á»ng dáº«n, tÃªn model).
    2. Láº¥y API Key.
    3. Gá»i cÃ¡c hÃ m trong PHáº¦N 2 Ä‘á»ƒ khá»Ÿi táº¡o táº¥t cáº£ thÃ nh pháº§n.
    4. Kiá»ƒm tra lá»—i.
    5. Äáº·t cÃ¢u há»i vÃ  gá»i hÃ m pipeline (PHáº¦N 3).
    6. In káº¿t quáº£ cuá»‘i cÃ¹ng.
    """
    
    # --- 1. Thiáº¿t láº­p háº±ng sá»‘ ---
    print("--- âš™ï¸ Báº®T Äáº¦U KHá»žI CHáº Y RAG PIPELINE âš™ï¸ ---")
    DB_PATH = "/content/drive/MyDrive/DL_RAG_Video_main/my_rag_db_2"
    COLLECTION_NAME = "bai_giang_videos"
    EMBEDDING_MODEL_NAME = 'VoVanPhuc/sup-SimCSE-VietNamese-phobert-base'
    RERANKER_MODEL_NAME = 'cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
    DRIVE_MOUNT_PATH = '/content/drive'

    # --- 2. Láº¥y API Key (An toÃ n) ---
    GEMINI_API_KEY = 'AIzaSyBD27hwT7Zu1yACDlbR1sEoVDKww2T2Cuo'
    if ON_COLAB:
        try:
            # Láº¥y key tá»« Colab Secrets (biá»ƒu tÆ°á»£ng chÃ¬a khÃ³a ðŸ”‘)
            GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY') 
        except Exception as e:
            print(f"KhÃ´ng thá»ƒ láº¥y 'GOOGLE_API_KEY' tá»« Colab Secrets: {e}")
    else:
        # Láº¥y key tá»« biáº¿n mÃ´i trÆ°á»ng náº¿u cháº¡y local
        GEMINI_API_KEY = os.environ.get('GOOGLE_API_KEY')

    if not GEMINI_API_KEY:
        print("\nâŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y 'GOOGLE_API_KEY'.")
        print("Vui lÃ²ng thiáº¿t láº­p biáº¿n nÃ y trong Colab Secrets hoáº·c mÃ´i trÆ°á»ng cá»§a báº¡n.")
        return

    # --- 3. Khá»Ÿi táº¡o táº¥t cáº£ thÃ nh pháº§n ---
    
    # Káº¿t ná»‘i Drive (cáº§n thiáº¿t náº¿u DB_PATH á»Ÿ trÃªn Drive)
    if ON_COLAB:
        ket_noi_google_drive(DRIVE_MOUNT_PATH)
    
    # Táº£i cÃ¡c model vÃ  káº¿t ná»‘i DB
    # (ChÃºng ta gá»i tuáº§n tá»± Ä‘á»ƒ log dá»… Ä‘á»c hÆ¡n)
    model_embed = tai_model_embedding(EMBEDDING_MODEL_NAME)
    model_rerank = tai_model_reranker(RERANKER_MODEL_NAME)
    rag_collection = ket_noi_chromadb(DB_PATH, COLLECTION_NAME)
    llm_model = thiet_lap_gemini(GEMINI_API_KEY)

    # --- 4. Kiá»ƒm tra ---
    all_components = {
        "Embedding Model": model_embed,
        "Reranker Model": model_rerank,
        "ChromaDB Collection": rag_collection,
        "LLM Model": llm_model
    }
    
    if not all(all_components.values()):
        print("\nâŒ Lá»–I: Má»™t hoáº·c nhiá»u thÃ nh pháº§n khÃ´ng thá»ƒ khá»Ÿi táº¡o.")
        for name, component in all_components.items():
            if not component:
                print(f"    - {name}: KHá»žI Táº O THáº¤T Báº I")
        print("Vui lÃ²ng kiá»ƒm tra láº¡i lá»—i bÃªn trÃªn vÃ  Ä‘Æ°á»ng dáº«n. Dá»«ng chÆ°Æ¡ng trÃ¬nh.")
        return
        
    print("\n===================================")
    print("âœ… Táº¤T Cáº¢ MODEL VÃ€ DB ÄÃƒ Sáº´N SÃ€NG!")
    print("===================================")

    # --- 5. Äáº·t cÃ¢u há»i vÃ  cháº¡y pipeline ---
    
    # â–¼â–¼â–¼ ÄÃ‚Y LÃ€ NÆ I Báº N Äáº¶T CÃ‚U Há»ŽI Cá»¦A MÃŒNH â–¼â–¼â–¼
    query = "Attention khÃ¡c Self-Attention á»Ÿ Ä‘iá»ƒm nÃ o"
    
    result = get_rag_answer_pipeline(
        original_query_text=query,
        llm_model=llm_model,
        embedding_model=model_embed,
        collection=rag_collection,
        reranker=model_rerank
    )

    # --- 6. In káº¿t quáº£ cuá»‘i cÃ¹ng ---
    print("\n\n================ Káº¾T QUáº¢ CUá»I CÃ™NG ================")
    print(f"â“ Há»ŽI: {query}\n")
    print(f"ðŸ¤– TRáº¢ Lá»œI:\n{result.get('answer', 'KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i')}\n")
    print(f"ðŸ”— NGUá»’N THAM KHáº¢O:\n{result.get('source_url', 'KhÃ´ng cÃ³ nguá»“n')}")
    print("==================================================")

# =======================================================
# PHáº¦N 5: ÄIá»‚M Báº®T Äáº¦U CHáº Y CODE
# =======================================================
if __name__ == "__main__":
    main()